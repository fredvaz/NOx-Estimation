+-----+------+----------------+----------------+----------------+----------------------+
| Top |  u1  |       w1       |       w2       |       b        |         MSE          |
+-----+------+----------------+----------------+----------------+----------------------+
|  1  | relu | random_normal  | random_normal  | random_uniform | 0.00832213132261214  |
|  2  | selu | random_uniform |      ones      | random_normal  | 0.00872079964557832  |
|  3  | relu | random_normal  | random_uniform | random_normal  | 0.008904837569306519 |
|  4  | selu | random_normal  |      ones      | random_normal  | 0.008967452894218943 |
|  5  | relu | random_uniform |      ones      | random_uniform | 0.009084382602436976 |
|  6  | relu | random_normal  |      ones      | random_normal  | 0.009159850612790748 |
|  7  | relu | random_normal  | random_normal  | random_normal  | 0.009177056077698415 |
|  8  | relu | random_uniform | random_normal  | random_uniform | 0.00921173021197319  |
|  9  | selu | random_uniform |      ones      |     zeros      | 0.009221820684615523 |
|  10 | relu | random_uniform |      ones      |     zeros      | 0.009270019659941847 |
+-----+------+----------------+----------------+----------------+----------------------+


  # Hidden node activation function
  u_acti = ['relu','selu'] 

  # Output node activation function
  u2 = 'relu' # better: relu > linear > selu > elu

  # Initial weights
  w_init = ['ones','random_normal','random_uniform']

  # Initial bias
  b_init = ['zeros','random_uniform','random_normal']
